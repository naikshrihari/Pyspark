{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-26808657744c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PYSPARK_PYTHON'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Library/Frameworks/Python.framework/Versions/3.6/bin/python3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/Library/Frameworks/Python.framework/Versions/3.6/bin/python3'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"rdd_demo\").getOrCreate()\n",
    "\n",
    "#sc.stop()\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MinTemperatures\")\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "[Row(dest='United States', source='Romania', count='15')]\n"
     ]
    }
   ],
   "source": [
    "##### read using datasource API\n",
    "\n",
    "flightData2015 = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"false\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(\"/Users/aakash/training/spark/data/flight-data/csv/2015-summary.csv\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "flightData2015=flightData2015.toDF(\"dest\",\"source\",\"count\").rdd\n",
    "print(type(flightData2015))\n",
    "print(flightData2015.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "['DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count', 'United States,Romania,15']\n"
     ]
    }
   ],
   "source": [
    "##### read using sparkcontext\n",
    "spth=\"/Users/aakash/training/spark/data/flight-data/csv/2015-summary.csv\"\n",
    "sc_flightData2015=spark.sparkContext.textFile(spth)\n",
    "print(type(sc_flightData2015))\n",
    "print(sc_flightData2015.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "  DEST_COUNTRY_NAME ORIGIN_COUNTRY_NAME  count\n",
      "0     United States             Romania     15\n",
      "1     United States             Croatia      1\n",
      "2     United States             Ireland    344\n",
      "3             Egypt       United States     15\n",
      "4     United States               India     62\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)]\n"
     ]
    }
   ],
   "source": [
    "##### convert pandas file to RDD\n",
    "import pandas as pd\n",
    "\n",
    "spth=\"/Users/aakash/training/spark/data/flight-data/csv/2015-summary.csv\"\n",
    "pd_flightData2015=pd.read_csv(spth, header=0)\n",
    "print(type(pd_flightData2015))\n",
    "print(pd_flightData2015.head())\n",
    "pd_flightData2015=spark.createDataFrame(pd_flightData2015).rdd\n",
    "print(type(pd_flightData2015))\n",
    "print(pd_flightData2015.take(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'The', 'Definitive', 'Guide', ':']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\\\n",
    "  .split(\" \")\n",
    "words = spark.sparkContext.parallelize(myCollection, 2)\n",
    "words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "##### from a collection of text\n",
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\\\n",
    "  .split(\" \")\n",
    "words = spark.sparkContext.parallelize(myCollection, 2)\n",
    "\n",
    "words.setName(\"myWords\")\n",
    "words.name() # myWords\n",
    "print(type(words))\n",
    "print(words.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### from a range of numbers\n",
    "myRange = spark.range(1000).toDF(\"number\").rdd.map(lambda row: row[0])\n",
    "#myRange = spark.range(1000).toDF(\"number\")\n",
    "myRange.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a look into some low level issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### some low level access issues\n",
    "pd_flightData2015.count()\n",
    "pd_flightData2015.take(2)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    date = fields[0]\n",
    "    p_open = fields[1]\n",
    "    p_close = fields[5]\n",
    "    return (date, p_open, p_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'Open', 'Close'), ('1998-03-23', '178.5', '180.2')]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spth=\"/Users/aakash/Sarasvati/NSE/RELIANCE.csv\"\n",
    "sdt=spark.sparkContext.textFile(spth)\n",
    "sdt=sdt.map(parseLine)\n",
    "sdt.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5366\n",
      "2525\n"
     ]
    }
   ],
   "source": [
    "spth=\"/Users/aakash/Sarasvati/NSE/RELIANCE.csv\"\n",
    "o_sdt = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .load(spth)\n",
    "o_sdt=o_sdt.toDF(\"Date\",\"Open\",\"High\",\"Low\",\"Last\",\"Close\",\"Volume\",\"Turnover\").rdd \\\n",
    "    .map(lambda row: (row[0], row[1], row[5]))\n",
    "print(o_sdt.count())\n",
    "#print(type(o_sdt))\n",
    "#print(o_sdt.take(2))\n",
    "o_sdt=o_sdt.filter(lambda row: row[2] > row[1])\n",
    "#print(o_sdt.take(5))\n",
    "#print(type(o_sdt))\n",
    "print(o_sdt.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HighClose(row):\n",
    "    ## Discuss\n",
    "    if row[2] > row[1]:\n",
    "        return(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything in output below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5366\n",
      "[('1998-03-23', '178.5', '180.2')]\n",
      "2536\n"
     ]
    }
   ],
   "source": [
    "o_sdt = spark.read.format(\"CSV\").option(\"header\",\"true\").load(spth)\n",
    "o_sdt=o_sdt.toDF(\"Date\",\"Open\",\"High\",\"Low\",\"Last\",\"Close\",\"Volume\",\"Turnover\").rdd.map(lambda row: (row[0], row[1], row[5]))\n",
    "print(o_sdt.count())\n",
    "o_sdt=o_sdt.filter(lambda row: HighClose(row))\n",
    "print(o_sdt.take(1))\n",
    "#print(type(o_sdt))\n",
    "print(o_sdt.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5366\n",
      "[(datetime.datetime(1998, 3, 23, 0, 0), 178.5, 180.2), (datetime.datetime(1998, 3, 24, 0, 0), 184.0, 178.7), (datetime.datetime(1998, 3, 25, 0, 0), 181.5, 183.85), (datetime.datetime(1998, 3, 26, 0, 0), 183.85, 179.45), (datetime.datetime(1998, 3, 27, 0, 0), 179.1, 180.4)]\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "5366\n"
     ]
    }
   ],
   "source": [
    "o_sdt = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(spth)\n",
    "o_sdt=o_sdt.toDF(\"Date\",\"Open\",\"High\",\"Low\",\"Last\",\"Close\",\"Volume\",\"Turnover\").rdd.map(lambda row: (row[0], row[1], row[5]))\n",
    "print(o_sdt.count())\n",
    "o_sdt=o_sdt.filter(lambda row: HighClose(row))\n",
    "print(o_sdt.take(5))\n",
    "print(type(o_sdt))\n",
    "print(o_sdt.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5366\n",
      "[(datetime.datetime(1998, 3, 23, 0, 0), 178.5, 180.2), (datetime.datetime(1998, 3, 25, 0, 0), 181.5, 183.85), (datetime.datetime(1998, 3, 27, 0, 0), 179.1, 180.4), (datetime.datetime(1998, 4, 1, 0, 0), 177.5, 182.85), (datetime.datetime(1998, 4, 3, 0, 0), 181.5, 184.95)]\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "2525\n"
     ]
    }
   ],
   "source": [
    "o_sdt = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(spth)\n",
    "o_sdt=o_sdt.toDF(\"Date\",\"Open\",\"High\",\"Low\",\"Last\",\"Close\",\"Volume\",\"Turnover\").rdd.map(lambda row: (row[0], row[1], row[5]))\n",
    "print(o_sdt.count())\n",
    "o_sdt=o_sdt.filter(lambda row: HighClose(row))\n",
    "print(o_sdt.take(5))\n",
    "print(type(o_sdt))\n",
    "print(o_sdt.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_to_mill(row):\n",
    "    return (row[0], row[1], row[2], round(row[3],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(2005, 10, 17, 0, 0), 380.0, 384.35, 17639.41), (datetime.datetime(2005, 10, 18, 0, 0), 386.85, 374.85, 13024.16)]\n",
      "[(datetime.datetime(2005, 10, 17, 0, 0), 380.0, 384.35, 17639.0), (datetime.datetime(2005, 10, 18, 0, 0), 386.85, 374.85, 13024.0)]\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "3461\n"
     ]
    }
   ],
   "source": [
    "spth=\"/Users/aakash/Sarasvati/NSE/TATASTEEL.csv\"\n",
    "o_sdt = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(spth)\n",
    "o_sdt=o_sdt.toDF(\"Date\",\"Open\",\"High\",\"Low\",\"Last\",\"Close\",\"Volume\",\"Turnover\").rdd.map(lambda row: (row[0], row[1], row[5], row[7]))\n",
    "print(o_sdt.take(2))\n",
    "o_sdt=o_sdt.map(to_to_mill)\n",
    "print(o_sdt.take(2))\n",
    "print(type(o_sdt))\n",
    "print(o_sdt.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', \"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\", '', 'This eBook is for the use of anyone anywhere at no cost and with', 'almost no restrictions whatsoever.  You may copy it, give it away or']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5703, 'the'), (3137, ''), (2882, 'and'), (2758, 'of'), (2720, 'to')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Func(lines):\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split(\" \")\n",
    "    return lines\n",
    "\n",
    "#sc.stop()\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"wordcount\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "spth=\"/Users/aakash/training/spark/data/sherlock_holmes.txt\"\n",
    "input_file = sc.textFile(\"data/sherlock_holmes.txt\")\n",
    "#print(input_file.take(5))\n",
    "rdd1 = input_file.flatMap(Func)\n",
    "rdd2=rdd1.map(lambda x: (x,1)).groupByKey().mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)\n",
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19900"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(1,200)).reduce(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110739"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110739"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.countApprox(1, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.defaultdict' object has no attribute 'countByValue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-6d487a664e77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountByValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.defaultdict' object has no attribute 'countByValue'"
     ]
    }
   ],
   "source": [
    "rdd1.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  408379', '  408212', '  408267', '  408337', '  408257', '  408238']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spth=\"/Users/aakash/training/spark/data/s1.txt\"\n",
    "spth_1=\"/Users/aakash/training/spark/data/sherlock_holmes.txt\"\n",
    "spth_2=\"/Users/aakash/training/spark/data/little_sherlock_holmes.txt\"\n",
    "input_file = sc.textFile(spth_1,6)\n",
    "\n",
    "\n",
    "input_file.pipe(\"wc -l\").collect()\n",
    "### why 2 outputs???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1514130"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "849690\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(type(input_file))\n",
    "print(input_file.count())\n",
    "print(input_file.getNumPartitions())\n",
    "print(sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file.saveAsTextFile('/Users/aakash/training/spark/wc.txt')\n",
    "## check outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
