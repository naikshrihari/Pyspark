{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/Library/Frameworks/Python.framework/Versions/3.6/bin/python3'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"streaming\").getOrCreate()\n",
    "\n",
    "#sc.stop()\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"streaming\")\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = spark.read.json(\"data/activity-data/\")\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Arrival_Time=1424686735090, Creation_Time=1424686733090638193, Device='nexus4_1', Index=18, Model='nexus4', User='g', gt='stand', x=0.0003356934, y=-0.0005645752, z=-0.018814087)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| gt|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+\n",
      "| gt|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|  stairsup|10461|\n",
      "|       sit|12308|\n",
      "|     stand|11384|\n",
      "|      walk|13255|\n",
      "|      bike|10797|\n",
      "|stairsdown| 9363|\n",
      "|      null|10446|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|  stairsup|10461|\n",
      "|       sit|12308|\n",
      "|     stand|11384|\n",
      "|      walk|13255|\n",
      "|      bike|10797|\n",
      "|stairsdown| 9363|\n",
      "|      null|10446|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|  stairsup|20913|\n",
      "|       sit|24617|\n",
      "|     stand|22769|\n",
      "|      walk|26511|\n",
      "|      bike|21594|\n",
      "|stairsdown|18728|\n",
      "|      null|20894|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "'Job aborted due to stage failure: Task 36 in stage 330.0 failed 1 times, most recent failure: Lost task 36.0 in stage 330.0 (TID 32187, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 644615 ms\\nDriver stacktrace:\\n=== Streaming Query ===\\nIdentifier: activity_counts_222 [id = e69056be-0a36-4f31-8a45-c4365dd7b037, runId = 995760ac-dfa8-4c49-ad0c-c9b382052556]\\nCurrent Committed Offsets: {FileStreamSource[file:/Users/aakash/training/spark/data/activity-data]: {\"logOffset\":78}}\\nCurrent Available Offsets: {FileStreamSource[file:/Users/aakash/training/spark/data/activity-data]: {\"logOffset\":79}}\\n\\nCurrent State: ACTIVE\\nThread State: RUNNABLE\\n\\nLogical Plan:\\nAggregate [gt#78], [gt#78, count(1) AS count#221L]\\n+- StreamingExecutionRelation FileStreamSource[file:/Users/aakash/training/spark/data/activity-data], [Arrival_Time#72L, Creation_Time#73L, Device#74, Index#75L, Model#76, User#77, gt#78, x#79, y#80, z#81]\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o163.awaitTermination.\n: org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 36 in stage 330.0 failed 1 times, most recent failure: Lost task 36.0 in stage 330.0 (TID 32187, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 644615 ms\nDriver stacktrace:\n=== Streaming Query ===\nIdentifier: activity_counts_222 [id = e69056be-0a36-4f31-8a45-c4365dd7b037, runId = 995760ac-dfa8-4c49-ad0c-c9b382052556]\nCurrent Committed Offsets: {FileStreamSource[file:/Users/aakash/training/spark/data/activity-data]: {\"logOffset\":78}}\nCurrent Available Offsets: {FileStreamSource[file:/Users/aakash/training/spark/data/activity-data]: {\"logOffset\":79}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nAggregate [gt#78], [gt#78, count(1) AS count#221L]\n+- StreamingExecutionRelation FileStreamSource[file:/Users/aakash/training/spark/data/activity-data], [Arrival_Time#72L, Creation_Time#73L, Device#74, Index#75L, Model#76, User#77, gt#78, x#79, y#80, z#81]\n\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:297)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:193)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 36 in stage 330.0 failed 1 times, most recent failure: Lost task 36.0 in stage 330.0 (TID 32187, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 644615 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2788)\n\tat org.apache.spark.sql.execution.streaming.MemorySink.addBatch(memory.scala:280)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5$$anonfun$apply$17.apply(MicroBatchExecution.scala:537)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch$5.apply(MicroBatchExecution.scala:535)\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.org$apache$spark$sql$execution$streaming$MicroBatchExecution$$runBatch(MicroBatchExecution.scala:534)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply$mcV$sp(MicroBatchExecution.scala:198)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1$$anonfun$apply$mcZ$sp$1.apply(MicroBatchExecution.scala:166)\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:351)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:58)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$runActivatedStream$1.apply$mcZ$sp(MicroBatchExecution.scala:166)\n\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:56)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:160)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:281)\n\t... 1 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d7aa961b2aa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mactivityQuery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.execution.QueryExecutionException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: 'Job aborted due to stage failure: Task 36 in stage 330.0 failed 1 times, most recent failure: Lost task 36.0 in stage 330.0 (TID 32187, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 644615 ms\\nDriver stacktrace:\\n=== Streaming Query ===\\nIdentifier: activity_counts_222 [id = e69056be-0a36-4f31-8a45-c4365dd7b037, runId = 995760ac-dfa8-4c49-ad0c-c9b382052556]\\nCurrent Committed Offsets: {FileStreamSource[file:/Users/aakash/training/spark/data/activity-data]: {\"logOffset\":78}}\\nCurrent Available Offsets: {FileStreamSource[file:/Users/aakash/training/spark/data/activity-data]: {\"logOffset\":79}}\\n\\nCurrent State: ACTIVE\\nThread State: RUNNABLE\\n\\nLogical Plan:\\nAggregate [gt#78], [gt#78, count(1) AS count#221L]\\n+- StreamingExecutionRelation FileStreamSource[file:/Users/aakash/training/spark/data/activity-data], [Arrival_Time#72L, Creation_Time#73L, Device#74, Index#75L, Model#76, User#77, gt#78, x#79, y#80, z#81]\\n'"
     ]
    }
   ],
   "source": [
    "## streaming version\n",
    "\n",
    "streaming = spark.readStream.schema(dataSchema)\\\n",
    ".option(\"maxFilesPerTrigger\", 1)\\\n",
    ".json(\"data/activity-data\")\n",
    "\n",
    "activityCounts = streaming.groupBy(\"gt\").count()\n",
    "\n",
    "activityQuery = activityCounts.writeStream \\\n",
    ".queryName(\"activity_counts_222\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()\n",
    "\n",
    "from time import sleep\n",
    "for x in range(5):\n",
    "    spark.sql(\"SELECT * FROM activity_counts_222\").show()\n",
    "    sleep(1)\n",
    "\n",
    "activityQuery.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+------+\n",
      "| gt|model|avg(x)|avg(y)|avg(z)|\n",
      "+---+-----+------+------+------+\n",
      "+---+-----+------+------+------+\n",
      "\n",
      "+---+-----+------+------+------+\n",
      "| gt|model|avg(x)|avg(y)|avg(z)|\n",
      "+---+-----+------+------+------+\n",
      "+---+-----+------+------+------+\n",
      "\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|        gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|      null|nexus4|-0.00534777761728...|-0.00471625131602...|0.001053548924143...|\n",
      "|      null|nexus4|0.001243822064214...|-0.00508318886798...|-0.01029860468302...|\n",
      "|      null|  null|0.001243822064214...|-0.00508318886798...|-0.01029860468302...|\n",
      "|      bike|nexus4| 0.02652570661093831| -0.0112163392482819|-0.08351623094110396|\n",
      "|     stand|  null|-1.91564553417079...|-1.20081210216091...|-6.67507354181460...|\n",
      "|       sit|nexus4|-5.48643225000002...|-1.75231850243742...|-2.21252465063370...|\n",
      "|     stand|nexus4|-1.91564553417079...|-1.20081210216091...|-6.67507354181460...|\n",
      "|stairsdown|  null|0.022206868836056676|-0.03261251395847493| 0.11849359875695864|\n",
      "|  stairsup|  null|-0.02502670570758...|-0.00196794930013...|-0.09861646979972294|\n",
      "|       sit|  null|-5.48643225000002...|-1.75231850243742...|-2.21252465063370...|\n",
      "|  stairsup|nexus4|-0.02502670570758...|-0.00196794930013...|-0.09861646979972294|\n",
      "|      walk|  null|-0.00533268257910...|0.007791046676386267|9.245203665258084E-4|\n",
      "|stairsdown|nexus4|0.022206868836056676|-0.03261251395847493| 0.11849359875695864|\n",
      "|      bike|  null| 0.02652570661093831| -0.0112163392482819|-0.08351623094110396|\n",
      "|      walk|nexus4|-0.00533268257910...|0.007791046676386267|9.245203665258084E-4|\n",
      "|      null|  null|-0.00534777761728...|-0.00471625131602...|0.001053548924143...|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|        gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|      null|nexus4|-0.00809936144458...|-7.23049259414186...|0.004338712656379912|\n",
      "|      null|nexus4|1.700410995007399E-4|-0.00493564422499...|-0.00916439323575...|\n",
      "|      null|  null|1.700410995007399E-4|-0.00493564422499...|-0.00916439323575...|\n",
      "|      bike|nexus4|  0.0250414329523943|-0.01079370949785...|-0.08251186291185036|\n",
      "|     stand|  null|-3.38293564662478...|-2.08204242566645...|3.235769281962317E-4|\n",
      "|       sit|nexus4|-5.76082048580253...|7.818385876020695E-5|-3.02597355932877...|\n",
      "|     stand|nexus4|-3.38293564662478...|-2.08204242566645...|3.235769281962317E-4|\n",
      "|stairsdown|  null| 0.02323278393461108| -0.0351078957245889| 0.12235328033884574|\n",
      "|  stairsup|  null|-0.02984970615695...|-0.00337107246117...|-0.09806757327446579|\n",
      "|       sit|  null|-5.76082048580253...|7.818385876020695E-5|-3.02597355932877...|\n",
      "|  stairsup|nexus4|-0.02984970615695...|-0.00337107246117...|-0.09806757327446579|\n",
      "|      walk|  null|-0.00505293905410...|0.007719333332326203|5.302249259137601E-4|\n",
      "|stairsdown|nexus4| 0.02323278393461108| -0.0351078957245889| 0.12235328033884574|\n",
      "|      bike|  null|  0.0250414329523943|-0.01079370949785...|-0.08251186291185036|\n",
      "|      walk|nexus4|-0.00505293905410...|0.007719333332326203|5.302249259137601E-4|\n",
      "|      null|  null|-0.00809936144458...|-7.23049259414186...|0.004338712656379912|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|        gt| model|              avg(x)|              avg(y)|              avg(z)|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "|      null|nexus4|-0.00982687243854...|-2.34653177343797...|0.003858852205067...|\n",
      "|      null|nexus4|3.918533547417218E-4|-0.00542182729197...|-0.00965178445029...|\n",
      "|      null|  null|3.918533547417218E-4|-0.00542182729197...|-0.00965178445029...|\n",
      "|      bike|nexus4|0.024714230810972924|-0.00958308822771315|-0.08257306722411313|\n",
      "|     stand|  null|-4.37317949751123...|1.750967655003801...|3.781399850939871...|\n",
      "|       sit|nexus4|-6.31601144100526...|1.898805381022023...|-1.87792168310448...|\n",
      "|     stand|nexus4|-4.37317949751123...|1.750967655003801...|3.781399850939871...|\n",
      "|stairsdown|  null|0.023922942892318545|-0.03527108296037...| 0.12134324810293659|\n",
      "|  stairsup|  null|-0.02695998701097...|-0.00516929759990...|-0.09870673527227804|\n",
      "|       sit|  null|-6.31601144100526...|1.898805381022023...|-1.87792168310448...|\n",
      "|  stairsup|nexus4|-0.02695998701097...|-0.00516929759990...|-0.09870673527227804|\n",
      "|      walk|  null|-0.00474912305724...|0.004750564660523073|-5.96844509898165E-4|\n",
      "|stairsdown|nexus4|0.023922942892318545|-0.03527108296037...| 0.12134324810293659|\n",
      "|      bike|  null|0.024714230810972924|-0.00958308822771315|-0.08257306722411313|\n",
      "|      walk|nexus4|-0.00474912305724...|0.004750564660523073|-5.96844509898165E-4|\n",
      "|      null|  null|-0.00982687243854...|-2.34653177343797...|0.003858852205067...|\n",
      "+----------+------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\\\n",
    ".drop(\"avg(Arrival_time)\")\\\n",
    ".drop(\"avg(Creation_Time)\")\\\n",
    ".drop(\"avg(Index)\")\\\n",
    ".writeStream.queryName(\"device_counts_1\").format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()\n",
    "\n",
    "\n",
    "from time import sleep\n",
    "for x in range(5):\n",
    "    spark.sql(\"SELECT * FROM device_counts_1\").show()\n",
    "    sleep(1)\n",
    "\n",
    "deviceModelStats.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
